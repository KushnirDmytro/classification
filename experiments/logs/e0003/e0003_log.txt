2019-08-08 23:20:31.394782

==== PARAMETERS:
experiment_name: e0003
seed: 42
with_cuda: True
path_save: experiments/logs/e0003
new_folder: True
TRAIN: 
  resume: 
  epochs: 150
  optim: sgd
  lr: 0.01
  momentum: 0.9
  weight_decay: 0.0005
  scheduler: StepLR
  lr_schedule_step: 40
  lr_schedule_gamma: 0.1
MODEL: 
  name: vgg_bn_pre_in3x32x32_out10
  init: default
  weights: 
DATASET: 
  name: cifar10
  path: data/cifar10/
  batch_size: 128
  batch_size_val: 256
  download: False
  transforms: 
    train: augment
    val: init
  tiny: False
LOG: 
  iter_interval: 10
  visdom: False
  tensorboard: True
  do_checkpoint: True
device: cuda


==== NET MODEL:
VggBnPre3x32x32(
  (vgg16): VGG(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace)
      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (9): ReLU(inplace)
      (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (12): ReLU(inplace)
      (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (16): ReLU(inplace)
      (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (19): ReLU(inplace)
      (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (22): ReLU(inplace)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace)
      (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace)
      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace)
      (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (36): ReLU(inplace)
      (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (39): ReLU(inplace)
      (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (42): ReLU(inplace)
      (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
    (classifier): Sequential(
      (0): Linear(in_features=25088, out_features=4096, bias=True)
      (1): ReLU(inplace)
      (2): Dropout(p=0.5)
      (3): Linear(in_features=4096, out_features=4096, bias=True)
      (4): ReLU(inplace)
      (5): Dropout(p=0.5)
      (6): Linear(in_features=4096, out_features=1000, bias=True)
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=10, bias=True)
  )
)
==== OPTIMIZER:
SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.01
    lr: 0.01
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0005
)

==== SCHEDULER:
<torch.optim.lr_scheduler.StepLR object at 0x7f26c11f4550>

==== DATASET (TRAIN):
Dataset CIFAR10
    Number of datapoints: 50000
    Root location: data/cifar10/
    Split: Train

==== DATASET (VAL):
Dataset CIFAR10
    Number of datapoints: 10000
    Root location: data/cifar10/
    Split: Test

[e0003] Epoch[  0  / 150 ] Iteration[  0  / 391 ] Loss: 18.2584 Time: 00:00:00:12
[e0003] Epoch[  0  / 150 ] Iteration[ 10  / 391 ] Loss: 2.9544 Time: 00:00:00:15
[e0003] Epoch[  0  / 150 ] Iteration[ 20  / 391 ] Loss: 2.3257 Time: 00:00:00:19
[e0003] Epoch[  0  / 150 ] Iteration[ 30  / 391 ] Loss: 2.1816 Time: 00:00:00:22
[e0003] Epoch[  0  / 150 ] Iteration[ 40  / 391 ] Loss: 2.1223 Time: 00:00:00:25
[e0003] Epoch[  0  / 150 ] Iteration[ 50  / 391 ] Loss: 2.0684 Time: 00:00:00:29
[e0003] Epoch[  0  / 150 ] Iteration[ 60  / 391 ] Loss: 2.0047 Time: 00:00:00:32
[e0003] Epoch[  0  / 150 ] Iteration[ 70  / 391 ] Loss: 2.0354 Time: 00:00:00:35
[e0003] Epoch[  0  / 150 ] Iteration[ 80  / 391 ] Loss: 2.0714 Time: 00:00:00:39
[e0003] Epoch[  0  / 150 ] Iteration[ 90  / 391 ] Loss: 1.8131 Time: 00:00:00:42
[e0003] Epoch[  0  / 150 ] Iteration[ 100 / 391 ] Loss: 1.8318 Time: 00:00:00:46
[e0003] Epoch[  0  / 150 ] Iteration[ 110 / 391 ] Loss: 1.6771 Time: 00:00:00:49
[e0003] Epoch[  0  / 150 ] Iteration[ 120 / 391 ] Loss: 1.7411 Time: 00:00:00:52
[e0003] Epoch[  0  / 150 ] Iteration[ 130 / 391 ] Loss: 1.6359 Time: 00:00:00:56
[e0003] Epoch[  0  / 150 ] Iteration[ 140 / 391 ] Loss: 1.5176 Time: 00:00:00:59
[e0003] Epoch[  0  / 150 ] Iteration[ 150 / 391 ] Loss: 1.6558 Time: 00:00:01:03
[e0003] Epoch[  0  / 150 ] Iteration[ 160 / 391 ] Loss: 1.6328 Time: 00:00:01:06
[e0003] Epoch[  0  / 150 ] Iteration[ 170 / 391 ] Loss: 1.4435 Time: 00:00:01:10
[e0003] Epoch[  0  / 150 ] Iteration[ 180 / 391 ] Loss: 1.6022 Time: 00:00:01:13
[e0003] Epoch[  0  / 150 ] Iteration[ 190 / 391 ] Loss: 1.5528 Time: 00:00:01:17
[e0003] Epoch[  0  / 150 ] Iteration[ 200 / 391 ] Loss: 1.1279 Time: 00:00:01:20
[e0003] Epoch[  0  / 150 ] Iteration[ 210 / 391 ] Loss: 1.3798 Time: 00:00:01:23
[e0003] Epoch[  0  / 150 ] Iteration[ 220 / 391 ] Loss: 1.3361 Time: 00:00:01:27
[e0003] Epoch[  0  / 150 ] Iteration[ 230 / 391 ] Loss: 1.2273 Time: 00:00:01:30
[e0003] Epoch[  0  / 150 ] Iteration[ 240 / 391 ] Loss: 1.2468 Time: 00:00:01:34
[e0003] Epoch[  0  / 150 ] Iteration[ 250 / 391 ] Loss: 1.4527 Time: 00:00:01:37
[e0003] Epoch[  0  / 150 ] Iteration[ 260 / 391 ] Loss: 1.1696 Time: 00:00:01:41
[e0003] Epoch[  0  / 150 ] Iteration[ 270 / 391 ] Loss: 1.2081 Time: 00:00:01:44
[e0003] Epoch[  0  / 150 ] Iteration[ 280 / 391 ] Loss: 1.2051 Time: 00:00:01:48
[e0003] Epoch[  0  / 150 ] Iteration[ 290 / 391 ] Loss: 1.0802 Time: 00:00:01:51
[e0003] Epoch[  0  / 150 ] Iteration[ 300 / 391 ] Loss: 1.1191 Time: 00:00:01:54
[e0003] Epoch[  0  / 150 ] Iteration[ 310 / 391 ] Loss: 0.9876 Time: 00:00:01:58
[e0003] Epoch[  0  / 150 ] Iteration[ 320 / 391 ] Loss: 1.0874 Time: 00:00:02:01
[e0003] Epoch[  0  / 150 ] Iteration[ 330 / 391 ] Loss: 1.1716 Time: 00:00:02:05
[e0003] Epoch[  0  / 150 ] Iteration[ 340 / 391 ] Loss: 1.1283 Time: 00:00:02:08
[e0003] Epoch[  0  / 150 ] Iteration[ 350 / 391 ] Loss: 0.8893 Time: 00:00:02:12
[e0003] Epoch[  0  / 150 ] Iteration[ 360 / 391 ] Loss: 1.0895 Time: 00:00:02:15
[e0003] Epoch[  0  / 150 ] Iteration[ 370 / 391 ] Loss: 1.2747 Time: 00:00:02:18
[e0003] Epoch[  0  / 150 ] Iteration[ 380 / 391 ] Loss: 0.9898 Time: 00:00:02:22
[e0003] Epoch[  0  / 150 ] Iteration[ 390 / 391 ] Loss: 1.1306 Time: 00:00:02:25
Epoch [  1  ]: Train Avg accuracy: 71.0840; Train Avg loss: 0.8443 
               Valid Avg accuracy: 74.9300; Valid Avg loss: 0.7576 
               Time: 00:00:02:59
               BEST MODEL SAVED
[e0003] Epoch[  1  / 150 ] Iteration[  0  / 391 ] Loss: 0.7579 Time: 00:00:02:59
[e0003] Epoch[  1  / 150 ] Iteration[ 10  / 391 ] Loss: 0.8950 Time: 00:00:03:02
[e0003] Epoch[  1  / 150 ] Iteration[ 20  / 391 ] Loss: 1.0021 Time: 00:00:03:06
[e0003] Epoch[  1  / 150 ] Iteration[ 30  / 391 ] Loss: 0.8636 Time: 00:00:03:09
[e0003] Epoch[  1  / 150 ] Iteration[ 40  / 391 ] Loss: 0.8291 Time: 00:00:03:13
[e0003] Epoch[  1  / 150 ] Iteration[ 50  / 391 ] Loss: 1.0439 Time: 00:00:03:16
[e0003] Epoch[  1  / 150 ] Iteration[ 60  / 391 ] Loss: 0.7412 Time: 00:00:03:20
[e0003] Epoch[  1  / 150 ] Iteration[ 70  / 391 ] Loss: 0.8592 Time: 00:00:03:23
[e0003] Epoch[  1  / 150 ] Iteration[ 80  / 391 ] Loss: 0.7800 Time: 00:00:03:27
[e0003] Epoch[  1  / 150 ] Iteration[ 90  / 391 ] Loss: 0.7607 Time: 00:00:03:30
[e0003] Epoch[  1  / 150 ] Iteration[ 100 / 391 ] Loss: 0.8523 Time: 00:00:03:34
[e0003] Epoch[  1  / 150 ] Iteration[ 110 / 391 ] Loss: 0.7523 Time: 00:00:03:37
[e0003] Epoch[  1  / 150 ] Iteration[ 120 / 391 ] Loss: 0.9494 Time: 00:00:03:41
[e0003] Epoch[  1  / 150 ] Iteration[ 130 / 391 ] Loss: 0.6790 Time: 00:00:03:44
[e0003] Epoch[  1  / 150 ] Iteration[ 140 / 391 ] Loss: 0.9141 Time: 00:00:03:48
[e0003] Epoch[  1  / 150 ] Iteration[ 150 / 391 ] Loss: 0.7683 Time: 00:00:03:51
[e0003] Epoch[  1  / 150 ] Iteration[ 160 / 391 ] Loss: 0.6614 Time: 00:00:03:55
[e0003] Epoch[  1  / 150 ] Iteration[ 170 / 391 ] Loss: 0.7213 Time: 00:00:03:58
[e0003] Epoch[  1  / 150 ] Iteration[ 180 / 391 ] Loss: 0.6762 Time: 00:00:04:02
[e0003] Epoch[  1  / 150 ] Iteration[ 190 / 391 ] Loss: 0.6598 Time: 00:00:04:05
[e0003] Epoch[  1  / 150 ] Iteration[ 200 / 391 ] Loss: 0.7332 Time: 00:00:04:08
[e0003] Epoch[  1  / 150 ] Iteration[ 210 / 391 ] Loss: 0.6940 Time: 00:00:04:12
[e0003] Epoch[  1  / 150 ] Iteration[ 220 / 391 ] Loss: 0.7397 Time: 00:00:04:16
[e0003] Epoch[  1  / 150 ] Iteration[ 230 / 391 ] Loss: 0.7812 Time: 00:00:04:19
[e0003] Epoch[  1  / 150 ] Iteration[ 240 / 391 ] Loss: 0.6249 Time: 00:00:04:23
[e0003] Epoch[  1  / 150 ] Iteration[ 250 / 391 ] Loss: 0.7524 Time: 00:00:04:26
[e0003] Epoch[  1  / 150 ] Iteration[ 260 / 391 ] Loss: 0.5750 Time: 00:00:04:30
[e0003] Epoch[  1  / 150 ] Iteration[ 270 / 391 ] Loss: 0.8171 Time: 00:00:04:33
[e0003] Epoch[  1  / 150 ] Iteration[ 280 / 391 ] Loss: 0.7150 Time: 00:00:04:37
[e0003] Epoch[  1  / 150 ] Iteration[ 290 / 391 ] Loss: 0.6589 Time: 00:00:04:40
[e0003] Epoch[  1  / 150 ] Iteration[ 300 / 391 ] Loss: 0.6346 Time: 00:00:04:44
[e0003] Epoch[  1  / 150 ] Iteration[ 310 / 391 ] Loss: 0.4360 Time: 00:00:04:47
[e0003] Epoch[  1  / 150 ] Iteration[ 320 / 391 ] Loss: 0.7234 Time: 00:00:04:51
[e0003] Epoch[  1  / 150 ] Iteration[ 330 / 391 ] Loss: 0.6355 Time: 00:00:04:54
[e0003] Epoch[  1  / 150 ] Iteration[ 340 / 391 ] Loss: 0.6901 Time: 00:00:04:58
[e0003] Epoch[  1  / 150 ] Iteration[ 350 / 391 ] Loss: 0.6701 Time: 00:00:05:01
[e0003] Epoch[  1  / 150 ] Iteration[ 360 / 391 ] Loss: 0.6846 Time: 00:00:05:05
[e0003] Epoch[  1  / 150 ] Iteration[ 370 / 391 ] Loss: 0.6944 Time: 00:00:05:08
[e0003] Epoch[  1  / 150 ] Iteration[ 380 / 391 ] Loss: 0.7266 Time: 00:00:05:12
[e0003] Epoch[  1  / 150 ] Iteration[ 390 / 391 ] Loss: 0.4656 Time: 00:00:05:15
Epoch [  2  ]: Train Avg accuracy: 79.6000; Train Avg loss: 0.6114 
               Valid Avg accuracy: 82.2400; Valid Avg loss: 0.5436 
               Time: 00:00:06:02
               BEST MODEL SAVED
[e0003] Epoch[  2  / 150 ] Iteration[  0  / 391 ] Loss: 0.5972 Time: 00:00:06:03
[e0003] Epoch[  2  / 150 ] Iteration[ 10  / 391 ] Loss: 0.6120 Time: 00:00:06:06
[e0003] Epoch[  2  / 150 ] Iteration[ 20  / 391 ] Loss: 0.8172 Time: 00:00:06:09
[e0003] Epoch[  2  / 150 ] Iteration[ 30  / 391 ] Loss: 0.4357 Time: 00:00:06:13
[e0003] Epoch[  2  / 150 ] Iteration[ 40  / 391 ] Loss: 0.6347 Time: 00:00:06:16
[e0003] Epoch[  2  / 150 ] Iteration[ 50  / 391 ] Loss: 0.6750 Time: 00:00:06:20
[e0003] Epoch[  2  / 150 ] Iteration[ 60  / 391 ] Loss: 0.6412 Time: 00:00:06:23
[e0003] Epoch[  2  / 150 ] Iteration[ 70  / 391 ] Loss: 0.7385 Time: 00:00:06:26
[e0003] Epoch[  2  / 150 ] Iteration[ 80  / 391 ] Loss: 0.7350 Time: 00:00:06:30
[e0003] Epoch[  2  / 150 ] Iteration[ 90  / 391 ] Loss: 0.6061 Time: 00:00:06:33
[e0003] Epoch[  2  / 150 ] Iteration[ 100 / 391 ] Loss: 0.4714 Time: 00:00:06:37
[e0003] Epoch[  2  / 150 ] Iteration[ 110 / 391 ] Loss: 0.7232 Time: 00:00:06:40
[e0003] Epoch[  2  / 150 ] Iteration[ 120 / 391 ] Loss: 0.5651 Time: 00:00:06:43
[e0003] Epoch[  2  / 150 ] Iteration[ 130 / 391 ] Loss: 0.5767 Time: 00:00:06:47
[e0003] Epoch[  2  / 150 ] Iteration[ 140 / 391 ] Loss: 0.7428 Time: 00:00:06:51
[e0003] Epoch[  2  / 150 ] Iteration[ 150 / 391 ] Loss: 0.6901 Time: 00:00:06:54
[e0003] Epoch[  2  / 150 ] Iteration[ 160 / 391 ] Loss: 0.6849 Time: 00:00:06:58
[e0003] Epoch[  2  / 150 ] Iteration[ 170 / 391 ] Loss: 0.5761 Time: 00:00:07:01
[e0003] Epoch[  2  / 150 ] Iteration[ 180 / 391 ] Loss: 0.5362 Time: 00:00:07:05
[e0003] Epoch[  2  / 150 ] Iteration[ 190 / 391 ] Loss: 0.4965 Time: 00:00:07:08
[e0003] Epoch[  2  / 150 ] Iteration[ 200 / 391 ] Loss: 0.5642 Time: 00:00:07:11
[e0003] Epoch[  2  / 150 ] Iteration[ 210 / 391 ] Loss: 0.5070 Time: 00:00:07:15
[e0003] Epoch[  2  / 150 ] Iteration[ 220 / 391 ] Loss: 0.5287 Time: 00:00:07:19
[e0003] Epoch[  2  / 150 ] Iteration[ 230 / 391 ] Loss: 0.5651 Time: 00:00:07:22
[e0003] Epoch[  2  / 150 ] Iteration[ 240 / 391 ] Loss: 0.5416 Time: 00:00:07:26
[e0003] Epoch[  2  / 150 ] Iteration[ 250 / 391 ] Loss: 0.5325 Time: 00:00:07:29
[e0003] Epoch[  2  / 150 ] Iteration[ 260 / 391 ] Loss: 0.6621 Time: 00:00:07:33
[e0003] Epoch[  2  / 150 ] Iteration[ 270 / 391 ] Loss: 0.5711 Time: 00:00:07:36
[e0003] Epoch[  2  / 150 ] Iteration[ 280 / 391 ] Loss: 0.5171 Time: 00:00:07:40
[e0003] Epoch[  2  / 150 ] Iteration[ 290 / 391 ] Loss: 0.5678 Time: 00:00:07:43
[e0003] Epoch[  2  / 150 ] Iteration[ 300 / 391 ] Loss: 0.6030 Time: 00:00:07:47
[e0003] Epoch[  2  / 150 ] Iteration[ 310 / 391 ] Loss: 0.5133 Time: 00:00:07:51
[e0003] Epoch[  2  / 150 ] Iteration[ 320 / 391 ] Loss: 0.4954 Time: 00:00:07:54
[e0003] Epoch[  2  / 150 ] Iteration[ 330 / 391 ] Loss: 0.4971 Time: 00:00:07:58
[e0003] Epoch[  2  / 150 ] Iteration[ 340 / 391 ] Loss: 0.4972 Time: 00:00:08:01
[e0003] Epoch[  2  / 150 ] Iteration[ 350 / 391 ] Loss: 0.6052 Time: 00:00:08:05
[e0003] Epoch[  2  / 150 ] Iteration[ 360 / 391 ] Loss: 0.4677 Time: 00:00:08:08
[e0003] Epoch[  2  / 150 ] Iteration[ 370 / 391 ] Loss: 0.4983 Time: 00:00:08:12
[e0003] Epoch[  2  / 150 ] Iteration[ 380 / 391 ] Loss: 0.4522 Time: 00:00:08:15
[e0003] Epoch[  2  / 150 ] Iteration[ 390 / 391 ] Loss: 0.4671 Time: 00:00:08:18
Epoch [  3  ]: Train Avg accuracy: 82.9520; Train Avg loss: 0.5111 
               Valid Avg accuracy: 84.5500; Valid Avg loss: 0.4767 
               Time: 00:00:09:08
               BEST MODEL SAVED
[e0003] Epoch[  3  / 150 ] Iteration[  0  / 391 ] Loss: 0.4844 Time: 00:00:09:09
[e0003] Epoch[  3  / 150 ] Iteration[ 10  / 391 ] Loss: 0.6445 Time: 00:00:09:12
[e0003] Epoch[  3  / 150 ] Iteration[ 20  / 391 ] Loss: 0.5586 Time: 00:00:09:16
[e0003] Epoch[  3  / 150 ] Iteration[ 30  / 391 ] Loss: 0.7036 Time: 00:00:09:19
[e0003] Epoch[  3  / 150 ] Iteration[ 40  / 391 ] Loss: 0.4577 Time: 00:00:09:22
[e0003] Epoch[  3  / 150 ] Iteration[ 50  / 391 ] Loss: 0.4020 Time: 00:00:09:26
[e0003] Epoch[  3  / 150 ] Iteration[ 60  / 391 ] Loss: 0.3562 Time: 00:00:09:29
[e0003] Epoch[  3  / 150 ] Iteration[ 70  / 391 ] Loss: 0.5213 Time: 00:00:09:32
[e0003] Epoch[  3  / 150 ] Iteration[ 80  / 391 ] Loss: 0.5454 Time: 00:00:09:36
[e0003] Epoch[  3  / 150 ] Iteration[ 90  / 391 ] Loss: 0.5268 Time: 00:00:09:39
[e0003] Epoch[  3  / 150 ] Iteration[ 100 / 391 ] Loss: 0.4054 Time: 00:00:09:43
[e0003] Epoch[  3  / 150 ] Iteration[ 110 / 391 ] Loss: 0.6479 Time: 00:00:09:46
[e0003] Epoch[  3  / 150 ] Iteration[ 120 / 391 ] Loss: 0.4126 Time: 00:00:09:50
[e0003] Epoch[  3  / 150 ] Iteration[ 130 / 391 ] Loss: 0.4686 Time: 00:00:09:53
[e0003] Epoch[  3  / 150 ] Iteration[ 140 / 391 ] Loss: 0.6447 Time: 00:00:09:57
[e0003] Epoch[  3  / 150 ] Iteration[ 150 / 391 ] Loss: 0.5435 Time: 00:00:10:00
[e0003] Epoch[  3  / 150 ] Iteration[ 160 / 391 ] Loss: 0.4984 Time: 00:00:10:04
[e0003] Epoch[  3  / 150 ] Iteration[ 170 / 391 ] Loss: 0.4945 Time: 00:00:10:07
[e0003] Epoch[  3  / 150 ] Iteration[ 180 / 391 ] Loss: 0.6773 Time: 00:00:10:11
[e0003] Epoch[  3  / 150 ] Iteration[ 190 / 391 ] Loss: 0.4263 Time: 00:00:10:14
[e0003] Epoch[  3  / 150 ] Iteration[ 200 / 391 ] Loss: 0.4845 Time: 00:00:10:18
[e0003] Epoch[  3  / 150 ] Iteration[ 210 / 391 ] Loss: 0.4950 Time: 00:00:10:21
[e0003] Epoch[  3  / 150 ] Iteration[ 220 / 391 ] Loss: 0.3941 Time: 00:00:10:25
[e0003] Epoch[  3  / 150 ] Iteration[ 230 / 391 ] Loss: 0.6088 Time: 00:00:10:28
[e0003] Epoch[  3  / 150 ] Iteration[ 240 / 391 ] Loss: 0.5407 Time: 00:00:10:32
[e0003] Epoch[  3  / 150 ] Iteration[ 250 / 391 ] Loss: 0.5261 Time: 00:00:10:35
[e0003] Epoch[  3  / 150 ] Iteration[ 260 / 391 ] Loss: 0.3950 Time: 00:00:10:39
[e0003] Epoch[  3  / 150 ] Iteration[ 270 / 391 ] Loss: 0.4674 Time: 00:00:10:42
[e0003] Epoch[  3  / 150 ] Iteration[ 280 / 391 ] Loss: 0.4448 Time: 00:00:10:46
[e0003] Epoch[  3  / 150 ] Iteration[ 290 / 391 ] Loss: 0.3865 Time: 00:00:10:49
[e0003] Epoch[  3  / 150 ] Iteration[ 300 / 391 ] Loss: 0.5239 Time: 00:00:10:53
[e0003] Epoch[  3  / 150 ] Iteration[ 310 / 391 ] Loss: 0.4174 Time: 00:00:10:56
[e0003] Epoch[  3  / 150 ] Iteration[ 320 / 391 ] Loss: 0.5589 Time: 00:00:11:00
[e0003] Epoch[  3  / 150 ] Iteration[ 330 / 391 ] Loss: 0.3681 Time: 00:00:11:04
[e0003] Epoch[  3  / 150 ] Iteration[ 340 / 391 ] Loss: 0.4471 Time: 00:00:11:07
[e0003] Epoch[  3  / 150 ] Iteration[ 350 / 391 ] Loss: 0.5937 Time: 00:00:11:11
[e0003] Epoch[  3  / 150 ] Iteration[ 360 / 391 ] Loss: 0.5596 Time: 00:00:11:14
[e0003] Epoch[  3  / 150 ] Iteration[ 370 / 391 ] Loss: 0.4273 Time: 00:00:11:18
[e0003] Epoch[  3  / 150 ] Iteration[ 380 / 391 ] Loss: 0.3942 Time: 00:00:11:21
[e0003] Epoch[  3  / 150 ] Iteration[ 390 / 391 ] Loss: 0.3959 Time: 00:00:11:25
Epoch [  4  ]: Train Avg accuracy: 83.7000; Train Avg loss: 0.4855 
               Valid Avg accuracy: 84.2900; Valid Avg loss: 0.4805 
               Time: 00:00:11:56
[e0003] Epoch[  4  / 150 ] Iteration[  0  / 391 ] Loss: 0.4953 Time: 00:00:11:57
[e0003] Epoch[  4  / 150 ] Iteration[ 10  / 391 ] Loss: 0.4600 Time: 00:00:12:01
[e0003] Epoch[  4  / 150 ] Iteration[ 20  / 391 ] Loss: 0.3513 Time: 00:00:12:04
